The first step was loading of caret:

<code>
library(caret)
</code>

Next, the training set was loaded:

<code>
training <- read.csv("pml-training.csv", header=T, na=c("NA",""))
</code>

Then I explored data to check meaning of columns:

<code>
dim(training)
training$X
table(training$user_name)
plot(training$raw_timestamp_part_1)
plot(training$raw_timestamp_part_2)
plot(training$cvtd_timestamp)
plot(training$new_window)
plot(training$num_window)
table(training$classe)
</code>

it turned out that columns 1:7 are useless for machine learning, so they were removed:

<code>
training.filt <- training[,8:ncol(training)]
</code>

Data contain a lot of missing values, columns with missing values were removed:

<code>
isna <- is.na(training.filt)
isna.col <- apply(isna, 2, sum)==0
training.pure <- training.filt[,isna.col]
</code>

The training set was paritioned to "real" training set and in-training validation set:

<code>
set.seed(666)
train.logical <-createDataPartition(training.pure$classe,p=0.5,list=FALSE)
real.training <- training.pure[train.logical,]
first.testing <- training.pure[-train.logical,]
</code>

It was trained by Random Forest with cross-validation:

<code>
set.seed(667)
rfmod <- train(classe~., data=real.training,
               method="rf", trControl=trainControl(method="cv"))
</code>

The result was

<code>
rfmod
#
rf_model$finalModel
#
</code>

plot(rf_model$finalModel)
first.pred <- predict(rfmod, first.testing)
cm<-confusionMatrix(first.pred, first.testing$classe)
image(1:5, 1:5, cm$table, col=rainbow(100)[70:1], zlim=c(0,3000),
      axes=F, xlab="predicted class", ylab="real class")
axis(1, labels=c("A","B","C","D","E"), at=1:5)
axis(2, labels=c("A","B","C","D","E"), at=1:5)
xpos <- rep(1:5, times=5)
ypos <- rep(1:5, each=5)
text(xpos, ypos, labels=as.vector(cm$table))

testing <- read.csv("pml-testing.csv", header=T)
